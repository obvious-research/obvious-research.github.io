<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta content="DESCRIPTION META TAG" name="description">
    <meta content="SOCIAL MEDIA TITLE TAG" property="og:title"/>
    <meta content="SOCIAL MEDIA DESCRIPTION TAG TAG" property="og:description"/>
    <meta content="URL OF THE WEBSITE" property="og:url"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta content="static/image/your_banner_image.png" property="og:image"/>
    <meta content="1200" property="og:image:width"/>
    <meta content="630" property="og:image:height"/>
    
    
    <meta content="TWITTER BANNER TITLE META TAG" name="twitter:title">
    <meta content="TWITTER BANNER DESCRIPTION META TAG" name="twitter:description">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta content="static/images/your_twitter_banner_image.png" name="twitter:image">
    <meta content="summary_large_image" name="twitter:card">
    <!-- Keywords for your paper to be indexed by-->
    <meta content="KEYWORDS SHOULD BE PLACED HERE" name="keywords">
    <meta content="width=device-width, initial-scale=1" name="viewport">
    
    
    <title>OnlyFlow</title>
    <link href="static/images/favicon.ico" rel="icon" type="image/x-icon">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">
    
    <link href="static/css/bulma.min.css" rel="stylesheet">
    <link href="static/css/bulma-carousel.min.css" rel="stylesheet">
    <link href="static/css/bulma-slider.min.css" rel="stylesheet">
    <link href="static/css/fontawesome.all.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
          rel="stylesheet">
    <link href="static/css/index.css" rel="stylesheet">
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title"><u>OnlyFlow</u>: Optical Flow based Motion Conditioning for
                        Video
                        Diffusion Models</h1>
                    <div class="is-size-5 publication-authors">
                        <!-- Paper authors -->
                        <span class="author-block">
                <a href="https://github.com/Arlaz" target="_blank">Mathis Koroglu</a><sup>1</sup>,</span>
                        <span class="author-block">
                  <a href="https://sites.google.com/view/hugo-caselles-dupre"
                     target="_blank">Hugo Caselles-Dupr√©</a><sup>1</sup>,</span>
                        <span class="author-block">
                    <a href="https://guillaumejs2403.github.io"
                       target="_blank">Guillaume Jeanneret Sanmiguel</a><sup>2</sup>,</span>
                        </span>
                        <span class="author-block">
                    <a href="https://cord.isir.upmc.fr" target="_blank">Matthieu Cord</a><sup>2</sup>
                  </span>
                    </div>
                    
                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup><a
                                href="https://obvious-research.com">Obvious Research</a> , <sup>2</sup><a
                                href="https://www.isir.upmc.fr">ISIR</a> - <a href="https://www.sorbonne-universite.fr/#Sciences">Sorbonne University, Paris</a></span>
                    </div>
                    
                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- Arxiv PDF link -->
                            <span class="link-block">
                        <a class="external-link button is-normal is-rounded is-dark"
                           href="https://arxiv.org/pdf/2411.10501.pdf"
                           target="_blank">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                            
<!--                            &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                            <span class="link-block">-->
<!--                      <a class="external-link button is-normal is-rounded is-dark"-->
<!--                         href="static/pdfs/supplementary_material.pdf"-->
<!--                         target="_blank">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->
                            
                            <!-- Github link -->
                            <span class="link-block">
                    <a class="external-link button is-normal is-rounded is-dark"
                       href="https://github.com/obvious-research/OnlyFlow"
                       target="_blank">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                            
                            <!-- ArXiv abstract Link -->
                            <span class="link-block">
                  <a class="external-link button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2411.10501"
                     target="_blank">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
                            <!-- HuggingFace link -->
                            <span class="link-block">
                    <a class="external-link button is-normal is-rounded is-dark"
                       href="https://github.com/obvious-research/OnlyFlow"
                       target="_blank">
                    <span class="icon">
                      <object data="https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.svg"
                              height="30" width="30"> </object>
                    </span>
                    <span>HuggingFace</span>
                  </a>
                </span>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <video autoplay playsinline height="100%" id="tree" loop muted poster="">
                <!-- Your video here -->
                <source src="static/videos/banner_video.mp4"
                        type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
                OnlyFlow controls the generation of video with text and the motion of a video input, using an estimation of its optical flow.
            </h2>
        </div>
    </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        We consider the problem of text-to-video generation tasks with precise control for various
                        applications such as camera movement control and video-to-video editing. Most methods tacking
                        this problem rely on providing user-defined controls, such as binary masks or camera movement
                        embeddings. In our approach we propose OnlyFlow, an approach leveraging the optical flow firstly
                        extracted from an input video to condition the motion of generated videos. Using a text prompt
                        and an input video, OnlyFlow allows the user to generate videos that respect the motion of the
                        input video as well as the text prompt. This is implemented through an optical flow estimation
                        model applied on the input video, which is then fed to a trainable optical flow encoder. The
                        output feature maps are then injected into the text-to-video backbone model. We perform
                        quantitative, qualitative and user preference studies to show that OnlyFlow positively compares
                        to state-of-the-art methods on a wide range of tasks, even though OnlyFlow was not specifically
                        trained for such tasks. OnlyFlow thus constitutes a versatile, lightweight yet efficient method
                        for controlling motion in text-to-video generation. Models and code will be made available on
                        GitHub and HuggingFace.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End paper abstract -->


<!-- strength video-->
<section class="hero">
    <div class="container is-max-desktop">
        <div class="hero-body">
                <h2 class="title">Conditioning strength</h2>
                <p>We can vary the influence of the optical flow conditioning strength for a more similar motion cloning.</p>
            <br>
                <video autoplay playsinline height="100%" id="tree" loop muted poster="">
                    <!-- Your video here -->
                    <source src="static/videos/strength.mov"
                            type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                    Illustration of optical flow conditioning strength impact on generated videos. The videos presented in each row are obtained for increasing values of gamma between 0 and 1.0                </h2>
        </div>
    </div>
</section>
<!-- End strength video -->

<!-- Camera control capabilities video-->
<section class="hero">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <h2 class="title">Camera control capabilities</h2>
            <p>OnlyFlow can serve as a tool to control camera movement using either another video or a preset motion field (optical flow) corresponding to a specific camera trajectory.</p>
            <br>
            <video autoplay playsinline height="100%" id="tree" loop muted poster="">
                <!-- Your video here -->
                <source src="static/videos/camerac_ctrl.mov"
                        type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
                All videos are generated using the same text prompt, camera trajectory (pan-left), and seed. Without having trained for this task, our model achieve the same camera control capability as other camera-movement approaches trained on this task.            </h2>
        </div>
    </div>
</section>
<!-- End Camera control capabilities video -->

<!-- comparison video-->
<section class="hero">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <h2 class="title">Comparison with other approaches</h2>
            <p>The model exhibits a superior combination of motion fidelity and image realism.</p>
            <br>
            <video autoplay playsinline height="100%" id="tree" loop muted poster="">
                <!-- Your video here -->
                <source src="static/videos/comparison.mp4"
                        type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
                Examples videos are generated using same text prompt and input video. It positively compares to approaches that uses depth map (RAVE, VideoComposer, Control-A-Video), is comparable to Gen-1's temporal coherence and VideoComposer's image quality.            </h2>
        </div>
    </div>
</section>
<!-- End comparison video -->

<!-- semantic_alignment video-->
<section class="hero">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <h2 class="title">Semantic alignment</h2>
            <p>Our model generate a video closely following both the prompt and optical flow</p>
            <br>
            <video autoplay playsinline height="100%" id="tree" loop muted poster="">
                <!-- Your video here -->
                <source src="static/videos/semantic_alignment.mov"
                        type="video/mp4">
            </video>
            <h2 class="subtitle has-text-centered">
                With the prompt "Trees in forest", our model generates a clothesline corresponding to the teeth of the smile.</h2>
        </div>
    </div>
</section>
<!-- End semantic_alignment video -->

<!-- Model architecture overview-->
<section class="hero ">
    <div class="hero-body">
        <div class="container">
            <h2 class="title">OnlyFlow model architecture</h2>
                    <!-- Your image here -->
            <div style="text-align: center;">
                <img src="static/images/archi.jpg" alt="OnlyFlow model architecture" width="65%" height="65%">
            </div>
                    <h2 class="subtitle has-text-centered">
                        Overview of OnlyFlow. Inputs are i) a tokenized and encoded text prompt, ii) noisy latents for the diffusion model and iii) the
                        optical flow of an input video. The latter is fed through a trainable optical flow encoder which outputs features maps that are injected in the
                        diffusion U-Net. We experiment with several injection strategies, for illustration purposes we only show the injection in temporal attention
                        layers of the U-Net. The U-Net is kept frozen during training. The output generated video matches the prompt and the auxiliary video‚Äôs
                        motion.
                    </h2>
        </div>
    </div>
</section>
<!-- End model architecture overview-->

<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@preprint{koroglu2024onlyflow,
      title={OnlyFlow: Optical Flow based Motion Conditioning for Video Diffusion Models},
      author={Mathis Koroglu and Hugo Caselles-Dupr√© and Guillaume Jeanneret Sanmiguel and Matthieu Cord},
      year={2024},
      eprint={2411.10501},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2411.10501},
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!--Acknowledgements -->
<section class="section" id="Acknowledgements">
    <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgements</h2>
        This project was provided with computing HPC & AI and storage resources by GENCI at IDRIS thanks to the grant 2024-AD011014329R1 on the supercomputer Jean Zay‚Äôs V100 & A100 partitions.
    </div>
</section>
<!--End Acknowledgements -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This page was built using the <a
                            href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic
                        Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io"
                                                                                target="_blank">Nerfies</a>¬†project
                        page.
                        You are free to borrow the source code of this website, we just ask that you link back to this
                        page in the footer. <br> This website is licensed under a <a
                            href="http://creativecommons.org/licenses/by-sa/4.0/"
                            rel="license"
                            target="_blank">Creative
                        Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>
</html>
